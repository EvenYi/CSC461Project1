Our dataset comes from Kaggle.com:
	1. Trending YouTube Video Statistics and Comments (https://www.kaggle.com/datasnaek/youtube#USvideos.csv)
	2. Youtube-video-dataset (https://www.kaggle.com/rahulanand0070/youtubevideodataset)
	
About our .csv files:
	We extract information such as tags and comments from the first dataset(Trending Youtube Video Statistics and Comments dataset) to form our Tags.csv and Comment.csv,
	which also uses our random value generation code to randomly generate user_id, video_id and release date.

	We extract video information such as title, url and description(used as intro in our database) from the second dataset(Youtube-video-dataset) to form Video.csv.
	User.csv is generated randomly with our username and password generation code.
	Following.csv is generated with random value generation code which randomly picks user_id and following_id and video_tag.csv is generated by randomly picking a tag from tags.csv and attaching it to a video.
	
	All user_id, video_id, tag_id, comment_id in reference relationships are dummy value, which created by uniform random generation, while authority is created by a non-uniform generation with a possibility of [0.3, 0.2, 0.2, 0.2, 0.1]

Since the value is created with uniform random possibility, there may exist duplicate rows that will be skipped during bulking load. 
Some emojis in comments or intros are encoded into utf-8.
We have the capability to generate as much data as we want, the video data can be large since the dataset we use is 15MB and comment information is 149MB, but this time we just generated 100 rows/table for experiment.

	